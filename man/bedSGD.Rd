% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gradient.R
\name{bedSGD}
\alias{bedSGD}
\title{Use the SGD optimiser to learn a linear regression model from plink data}
\usage{
bedSGD(
  bed,
  y,
  maxIter = 1000,
  p = list(),
  ssize = 100,
  seed = NULL,
  verbose = TRUE,
  iterskip = 10
)
}
\arguments{
\item{bed}{A \code{rbed} object as returned by \code{\link[pcapred]{readbed}}, or a \code{mergedrbed} object as returned by \code{\link[pcapred]{mergeref}}.}

\item{y}{A vector of length bed$no.ind, giving the predicted value}

\item{maxIter}{(default=1000) maximum number of iterations to run the optimiser for}

\item{p}{(default =list()) a vector specifying any control parameters to be set manually; these are described above.}

\item{ssize}{(default=100) Number of 4-individual binary chunks to use per stochastic update; this is the "minibatch" size.}

\item{seed}{(default=NULL) Optional seed setting}

\item{verbose}{(default=TRUE) whether to display iteration progress}

\item{iterskip}{(dfefault=10) If verbose, print to screen only on every iterskip iteration.}
}
\value{
A list containing:
\itemize{
\item loss: The history of the average loss (length maxIter), on the subset of data it was calculated on
\item change: The history of change size (length maxIter), i.e.  the RMS of the step sizes
\item theta: The current best estimate of \eqn{\beta}.
\item p: The previous parameter estimate.
}
}
\description{
For \eqn{y = \beta X + \epsilon}, use the SGD optimiser to minimise
\deqn{|\epsilon|^2_2= \sum{i=1}^N (y_i - \sum_{j=1}^L \beta_{j} X_{ij})^2.}
The SGD (Stochastic Gradient Descent) approach allows linear regression to be applied to very large problems for which exact solutions are impossible.

It is very good for large data problems but the learning rate needs to be carefully chosen.

The components of this model are (p below):
\itemize{
\item theta (if missing, generate randomly using \code{\link{initTheta}}) our estimate of \eqn{\beta} of length L.
\item alpha (default 0.1/bed$no.snp) The "learning rate". The algorithm will diverge if this is too large.
}
}
